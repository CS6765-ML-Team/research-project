\documentclass[12pt]{article}
\usepackage{graphicx}
    \graphicspath{content/images/}
\usepackage[a4paper, portrait, margin = 1in]{geometry}
\usepackage{times}
\usepackage{indentfirst}
\usepackage[square, numbers, sort&compress]{natbib}
\usepackage[titles]{tocloft}                          % package for table of contents 
    \advance\cftsecnumwidth -0.25em\relax                                 % decrease spacing between number and letter
    \renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}                 % table of contents setting 
    \renewcommand{\thesection}{\arabic{section}.}                         % table of contents setting - sections
    \renewcommand{\thesubsection}{\thesection\arabic{subsection}}         % table of contents setting - subsections
    \renewcommand{\thesubsubsection}{\thesection\arabic{subsubsection}.}  % table of contents setting - subsubsections
    \setlength{\cftbeforesecskip}{1pt}                                    % table of contents setting - idek
    \cftsetindents{sec}{0em}{1.25em}                                      % table of contents horizontal spacing - sections
    \cftsetindents{subsec}{1.25em}{6.5mm}                                 % table of contents horizontal spacing - subsections
    \cftsetindents{subsubsec}{2.5em}{7.5mm}                               % table of contents horizontal spacing - subsubsections
\usepackage{secdot}                                         % puts a dot in the number of section titles
\usepackage{titlesec}                                       % decreases spacing in titles
    \titlelabel{\thetitle\ }                                % decreases spacing in titles
    \usepackage [autostyle, english = american]{csquotes}   % fix the quotations
    \MakeOuterQuote{"}                                      % fix the quotations
\setlength{\parskip}{1em}  % Adjust the value as needed


\begin{document}

\setcounter{section}{2}
\section{\hspace{-3mm} 0 Challenges}

With such an impressive showing of studies wherein machine learning algorithms were leveraged within healthcare for the classification, 
inspection, and detection of several serious radiological, dermatological, haemotological and many other such diseases, it might come across as rather
surprising to learn that real world deployments of machine learning algorithms within clinical practice are rare \citep{Kelly_2019}. There are many reasons for this,
and it is these inherent challenges that plague the widespread adoption and deployment of machine learning algorithms into the field of healthcare as a whole. Within this section
several such key challenges will be highlighted and examined through both an ethical and legalistic lens, as well as pertaining to the disconnect between the design of machine learning algorithms 
and the understanding of those algorithms by medical practitioners. \

\vspace{-5mm}
\subsection{Adaptation to a Clinical Setting}

As outlined by Kelly et al. in \citep{Kelly_2019}, many challenges arise pertaining to the translation of machine learning models into clinical practice. While many studies have been performed that showcase 
seemingly impressive results within a clinical setting, in reality these studies have been retrospective in nature, that is they have used historically labelled data to train and test algorithms. In reality 
to best understand how these AI systems perform within a true clinical setting, \textit{prospective} studies must be performed. Prospective studies represent the dynamic nature of a clinical setting,
wherein patients are monitored as their medical characteristics or circumstances change as a function of time. In reality, the performance of these AI systems is likely to be worse in prospective studies 
as compared to retrospective studies, as the real-world data collected from a prospective study will vary substantially from the retrospective data used to train the algorithm \citep{Kelly_2019}. \

Another problem that presents itself relates to the nature of machine learning literature. As a common practice within the machine learning community, many studies that are performed are published first to pre-print servers, and not 
to peer-reviewed journals \citep{Kelly_2019}. Due to the stringent nature of the medical community, peer-reviewed evidence will be imperative for the trust and adoption of AI systems within the broader context of the healthcare industry.
The common, so called "gold-standard" of medical research is the use of randomized, controlled trials \citep{Kelly_2019,Khan_2023}, which is not common practice within the machine learning community. As such, high quality reporting of machine learning studies is required for the adoption of these systems into the healthcare industry. For clinicians and medical practitioners to trust and adopt these models into their daily use, there must exist clear and concise reporting on every aspect of these models, such that their usefullness within a clinical setting can be quantified \citep{Kelly_2019}. \

Clinicians and medical practitioners are foremost qualified in medicine, and are not well versed in the knowledge and terminology of machine learning models; as such, many of the metrics that are often utilized within the machine learning community have no tangible meaning to them. As outlined by Kelly et al., "accuracy does not necessarily represent clinical efficacy" \citep{Kelly_2019}. Area under the curve of a receiver operating characteristic curve is a widely used metric within the machine learning community, however it not necessarily the best metric to represent clinicial applicability as it is not easily understandable. \

\newpage

Similarly, other such metrics such as specificity and sensitivity recorded at a given model operating point, might have no meaning to a doctor who is not well versed in these commonly used terminologies. Arguably the most important factor is that these measure do not reflect what is most important to patients, namely whether the use of the model results in a beneficial change in the care of the patient \citep{Kelly_2019}. Therefore for ML algorithms to be widely adopted within the healthcare industry, these metrics must become more conducive of the clinical understanding of efficacy. Medical students and practicing clinicians should be properly trained in introductory machine learning courses to provide them with the knowledgebase to understand, digest, and adopt these models into their daily workflow.\

There are numerous other challenges related to the adoption of machine learning models within a clinical setting. There exists difficulty in comparing models due to the variable methodologies used on datasets with differing distributions. The "dataset-shift" problem also exists, where the introduction of AI models will generate a shift in medical practices, which will cause a change in the dataset used to train these models: effectively chasing a moving target. Current AI models are also far from generalisability, as there exist many unknown variables such as differences in equipment, laboratory equipment, electronic health record systems (EHRs), as well as differences in their codebases. Finally, there exist many logistical difficulties in implementing AI systems, as most healthcare data is not currently stored in a form that is easily digestable by these systems. Data is stored in numerous different mediums, such as medical imaging archival systems, pathology systems, EHRs, insurance databases, and other locations. The amalgamation of all of this data into a single, unifed dataset is a very difficult undertaking \citep{Kelly_2019}. \

When these AI systems make false decisions in automated diagnoses, the results can sometimes be quite harmful to those affected. Due to the nature of EHRs, the data that these machine learning algorithms are trained on is not always the highest quality, or contains numerous inaccuracies. These so called "data errors" can lead to potentially disastrous results, and these "decision errors" have tangible, real effects that affect real patients \citep{Ali_2023}. Similarly, Habehh \& Gohel highlight that the probabilty of error in prediction as well as the impact of this error are downsides of using these models within a clinical setting. Due to how these models rely on probablistic distributions, there is skepticism as to the validity of the predictions made by these models, and the implications of ML-based algorithms failing to operate as intended have severe and potentially fatal repercussions. Finally, they highlight how the rise of ML within the healthcare industry could fuel a decline in the personal relationship that many patients have between the healthcare practitioner, resulting in potential rejection from the public \citep{Habehh_2021}. For these aforementioned reasons, these ML models are far from widespread adaptation into clinical settings, from the perspective of healthcare practitioners and clinicians. \

\vspace{-5mm}
\subsection{The Black Box Problem}

Another challenge with the use of machine learning algorithms within the healthcare industry is that of the \textit{black box problem}. While this could fall under the section titled Clinical Adaptation, or perhaps the section titled Legal Issues, the author(s) felt it pertinent that this problem recieve its own dedicated section. While the black box problem does factor into why these models are not widely adopted within the healthcare industry from the viewpoint of a clinician, and it also has heavy legal implications, the problem is so inherently intertwined into the inner workings of machine learning that it was given its own section. \

The black box problem relates to the interpretability of a machine learning model, and ones own ability to understand and explain the reasoning as to why and how the output of the model was produced. At its core, this problem stems from how these models are formulated. Large swathes of input data are fed in to train the model to detect patterns that are sometimes anomalous in nature that would be otherwise difficult for a human practitioner to detect. These models then take unforeseen test data and either classify or make a decision based on the type of algorithm. But intrisinic to the functioning of a machine learning algorithm is the manner in which this learning takes place. While we can observe the input and output of the model, we have no way of understanding what is happening within the model itself: this is what is known as the hidden layers. \

These hidden layers learn new representations of the input, and each hidden layer is a re-representation of the input, as information is extracted at higher and higher levels of abstraction. As human operators we have no idea what occurs within the hidden layer. For a deep learning model with millions or even billions of parameters, one can imagine the complexity of the interactions between these parameters and the abstraction that occurs. Therefore the effectiveness of these AI models in the healthcare industry is limited entirely by their own inability to explain their decision making in a way that is understandable \citep{Kelly_2019, Guan_2019}. As one can imagine, this is entirely problematic within the medical industry. A patient is not going to agree to a treatment without first understanding the decision making process that went into determining which treatment was best for them. \

As outlined by Kelly et al., the best performing algorithms are unfortunately the least explainable (deep learning), whereas models with poorer performance are often more explainable (linear regression, decision trees) \citep{Kelly_2019}. If an algorithm ends up making an incorrect decision, or incorrectly recommends something to a healthcare practitioner and they act upon that decision, there is no legal basis for explaining or defending the system as we simply do not know what lead to that decision. This makes it difficult for scientists to understand how data correlates to their predictions, and jeopardizes the faith that the public has in the medical system \citep{Khan_2023}. Therefore the fact that these algorithms cannot be supervised by medical professionals represents a glaring challenge with their implementation and widespread adoption \citep{Murdoch_2021}.

\vspace{-5mm}
\subsection{Discrimination and Bias}

Another challenge that plagues the implementation of ML into the field of healthcare is that of discrimination and bias. As demonstrated by algorithms implemented into non-medical fields, problematic decisions have been
made by algorithms that reflect biases that are inherently woven into the data used to train them \citep{Char_2018,Khan_2023,Kelly_2019}. These biases stem from systemic problems with the collection of data, such as under-representation of minorities due to racial biases in dataset development, which in turn will lead to subpar prediction results \citep{Khan_2023}. Similarly, 
the implementation of AI systems affect groups that are disadvantaged by factors such as race, gender, and socioeconomic background. Such examples of this within the healthcare industry are mortality prediction algorithms
with accuracy that varies by ethnicity \citep{Chen_2018} and dermatological algorithms that classify both benign and malignant moles with accuracy that could rival a board-certified dermatologist, but with severe underperformance on images of these types of moles when tested on people of colour \citep{Esteva_2017,Haenssle_2019}. \ 

\newpage

The unfairness of an algorithm can be classified into three constituents, such as (1) model bias, where the model elects to best represent the majority but not the under-represented, marginalized groups; (2) model variance, due to inadequate data stemming from minority communities; and (3) outcome noise, such as the effect of unobserved variables that interact with the models predictions \citep{Chen_2018}. 
Algorithms may become biased if there have been no medical studies done in certain populations, and as such the data is lacking \citep{Char_2018}. The result of this is when the algorithm functions as intended, many people benefit. However when the decision making process falters, those that are negatively affected will be from marginalized communities that are under-represented within the very dataset used to train the algorithm, which compounds societal inequities \citep{Panch_2018}.


Price and Cohen outlined bias in action by considering the allocation of scarce medical resources amongst multiple patients. They propose a scenario wherein a particular minority group responds worse off to the proposed medical intervention then other groups, and as such failure to collect information on this minority group would potentially lead to the algorithm giving the minority group more priority than if the data had been included. If the minority group actually responds better to the intervention, the opposite effect could result \citep{Price_2019}.
It is therefore important that strategies to minimize the discrimination of marginalized communities and bias stemming from disparities in datasets between races be implemented throughout the development of machine learning algorithms for use in the healthcare sector.


A bias that was surprising to the author(s) was one that did not stem from the data itself, but rather the intent of those designing the algorithms, as highlighted by Char and Shah in \citep{Char_2018}. They highlight how the medical industry is entering into a new age of privatized development, wherein those responsible for designing ML algorithms for private sector healthcare corporations might be inclined towards malpractice to bolster their algorithm performance. There may exist temptation towards teaching machine learning systems to guide users towards clinical actions that would not necessarily lead to better care, but would lead to better quality metrics and hospital ratings. These systems might be designed in a manner that when they are being scrutinized by health regulators, they skew the data provided for public evaluation. The authors highlight Volkswagen's algorithm for passing emissions test by reducing their nitrogen oxide emissions when tested. Therefore, one must carefully consider the delivate balance between turning a profit and bolstering ratings, and providing improved healthcare for patients. Those responsible for desigining ML for use in healthcare are most likely not healthcare professionals themselves, and therefore need to seriously consider this tension.\

\vspace{-5mm}
\subsection{Data Privacy and Ethics}

Arguably the largest concern with the use of machine learning in the healthcare field is that of data privacy and ethics. As we all know, ML algorithms require copious amounts of data to sufficiently train them to a deployable state. In the context of healthcare, there are often issues with acquiring such a large repository of data. This stems from the fact that patient records are often regarded as confidential, and as such many healthcare organizations are not yet ready to offer up this personable information \citep{Khan_2023}. The deployment of ML systems into the healthcare field has vast implications towards the privacy and confidentiality of data. As early as 1982 some researchers within the medical field were describing the once Hippocratic cornerstone of medicine, confidentiality, as a decrepit and dated concept \citep{Siegler_1982}, an opinion that might be more relevant now than ever.\

Char and Shah highlight how the implementation of ML as a decision making support tool into the healthcare field will make confidentiality an increasingly difficult concept to maintain, as patients whose data are not recorded cannot benefit from machine learning analyses \citep{Char_2018}, and the training of ML models will require as much data as possible to ensure clinical accuracy. There also lies the issue in conglomerating health records from all over into one central location, as health records are important and vulnerable data that might be targeted by hackers. There must exist sufficient security measures to prevent data breaches and protect this vulnerable, confidential information \citep{Khan_2023,Ali_2023}. To protect against these kinds of breaches, under the Health Insurance Portability and Accountability Act (HIPAA) patient information is modified by removing a set of 18 specified identifiers, such as names and emails \citep{Price_2019}. Murdoch states in \citep{Murdoch_2021} however that several emerging computational strategies can be used to re-identify individuals in health data breaches, even if the information has been anonymized and scrubbed of identifiers, with success rates as high as 86\% \citep{Murdoch_2021,Check_Hayden_2013,Gymrek_2013,Erlich_2018,Ji_2019}.  \

% talk about consent (khan, gerke, guan, murdoch, etc.):

But what if a patient does not want their data to be used to train a ML model? Patient consent therefore becomes a glaring issue, with problems related to informed consent \citep{Gerke_2020,Guan_2019}, consent to data collection \citep{Gerke_2020,Ali_2023,Murdoch_2021}, privacy and consent \citep{Murdoch_2021}, and the extent of consent \citep{Racine_2019,Price_2019} being topics of debate. Informed consent in the context of data management refers to a persons right to know where and how their data is being used, as well as what kinds of data is being used. In terms of data collection, a patient should have consent as to whether or not their information can be used in the training of an ML model. In most recent years, Google has gotten into trouble with their predatory data collection tactics regarding healthcare data being harvested from patients unknowingly.\

In 2017 Google's DeepMind partnered with the British NHS Foundation to use machine learning as an assistive tool in the management of acute kidney injury. What users of this app did not know, however, was that their data was inappropriately being annexed from the United Kingdom to the United states, and this private data was collected without the consent or agency of the user \citep{Gerke_2020,Murdoch_2021}. As Murdoch states in \citep{Murdoch_2021}, corporations may not always be encouraged to maintain privacy protection if they can monetize the data or otherwise gain from it, provided that the legal penalties are not high enough to offset this behaviour. In a 2018 survey of four thousand American adults, it was found that only 11\% were willing to share their health data with tech companies, compared to 72\% with physicians. Similarly, only 31\% were either "somewhat confident" or "confident" in the abilities of tech companies to safeguard their data \citep{Murdoch_2021}.\


% I have talked about patient confidentiality, now I need to talk about the collection of data and the consent behind harvesting data and using public records. could talk about the NHS scandal with google deepmind, as well other such resources that talk about the scandals behind data collection.

% the ordering should be confidentiality -> data collection -> consent about data use and when can you opt out -> other topics

% can also maybe blend the legality side into this section. 


\subsection{Legal Issues}

\newpage
\bibliographystyle{ieeetr}
\bibliography{content/matt.bib}

\end{document}
