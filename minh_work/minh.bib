@article{price_privacy_2019,
	title = {Privacy in the age of medical big data},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {https://www.nature.com/articles/s41591-018-0272-7},
	doi = {10.1038/s41591-018-0272-7},
	language = {en},
	number = {1},
	urldate = {2024-10-04},
	journal = {Nature Medicine},
	author = {Price, W. Nicholson and Cohen, I. Glenn},
	month = jan,
	year = {2019},
	pages = {37--43},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\RH9N9GTP\\Price and Cohen - 2019 - Privacy in the age of medical big data.pdf:application/pdf},
}


@article{wu_machine_2019,
	title = {Machine learning for diagnostic ultrasound of triple-negative breast cancer},
	volume = {173},
	issn = {1573-7217},
	url = {https://doi.org/10.1007/s10549-018-4984-7},
	doi = {10.1007/s10549-018-4984-7},
	abstract = {Early diagnosis of triple-negative (TN) breast cancer is important due to its aggressive biological characteristics, poor clinical outcomes, and limited options for therapy. The goal of this study is to evaluate the potential of machine learning with quantitative ultrasound image features for the diagnosis of TN breast cancer.},
	language = {en},
	number = {2},
	urldate = {2024-10-16},
	journal = {Breast Cancer Research and Treatment},
	author = {Wu, Tong and Sultan, Laith R. and Tian, Jiawei and Cary, Theodore W. and Sehgal, Chandra M.},
	month = jan,
	year = {2019},
	keywords = {Breast cancer, Computer-aided diagnosis, Machine learning, Quantitative breast ultrasound, Triple negative breast cancer},
	pages = {365--373},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\3IFKX8IS\\Wu et al. - 2019 - Machine learning for diagnostic ultrasound of triple-negative breast cancer.pdf:application/pdf},
}

@inproceedings{van_ginneken_off--shelf_2015,
	title = {Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans},
	url = {https://ieeexplore.ieee.org/abstract/document/7163869},
	doi = {10.1109/ISBI.2015.7163869},
	abstract = {Convolutional neural networks (CNNs) have emerged as the most powerful technique for a range of different tasks in computer vision. Recent work suggested that CNN features are generic and can be used for classification tasks outside the exact domain for which the networks were trained. In this work we use the features from one such network, OverFeat, trained for object detection in natural images, for nodule detection in computed tomography scans. We use 865 scans from the publicly available LIDC data set, read by four thoracic radiologists. Nodule candidates are generated by a state-of-the-art nodule detection system. We extract 2D sagittal, coronal and axial patches for each nodule candidate and extract 4096 features from the penultimate layer of OverFeat and classify these with linear support vector machines. We show for various configurations that the off-the-shelf CNN features perform surprisingly well, but not as good as the dedicated detection system. When both approaches are combined, significantly better results are obtained than either approach alone. We conclude that CNN features have great potential to be used for detection tasks in volumetric medical data.},
	urldate = {2024-10-16},
	booktitle = {2015 {IEEE} 12th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {van Ginneken, Bram and Setio, Arnaud A. A. and Jacobs, Colin and Ciompi, Francesco},
	month = apr,
	year = {2015},
	note = {ISSN: 1945-8452},
	keywords = {Biomedical imaging, Cancer, computed tomography, Computed tomography, convolutional neural networks, Design automation, Feature extraction, Lesions, Lungs, Nodule detection},
	pages = {286--289},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\caomi\\Zotero\\storage\\62EPRISW\\7163869.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\LUKV5NDA\\van Ginneken et al. - 2015 - Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomog.pdf:application/pdf},
}

@misc{rajpurkar_chexnet_2017,
	title = {{CheXNet}: {Radiologist}-{Level} {Pneumonia} {Detection} on {Chest} {X}-{Rays} with {Deep} {Learning}},
	shorttitle = {{CheXNet}},
	url = {http://arxiv.org/abs/1711.05225},
	doi = {10.48550/arXiv.1711.05225},
	abstract = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
	month = dec,
	year = {2017},
	note = {arXiv:1711.05225 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\caomi\\Zotero\\storage\\K6NZCHJ4\\Rajpurkar et al. - 2017 - CheXNet Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\9LUZTCYX\\1711.html:text/html},
}

@inproceedings{arevalo_convolutional_2015,
	title = {Convolutional neural networks for mammography mass lesion classification},
	url = {https://ieeexplore.ieee.org/document/7318482/?arnumber=7318482},
	doi = {10.1109/EMBC.2015.7318482},
	abstract = {Feature extraction is a fundamental step when mammography image analysis is addressed using learning based approaches. Traditionally, problem dependent handcrafted features are used to represent the content of images. An alternative approach successfully applied in other domains is the use of neural networks to automatically discover good features. This work presents an evaluation of convolutional neural networks to learn features for mammography mass lesions before feeding them to a classification stage. Experimental results showed that this approach is a suitable strategy outperforming the state-of-the-art representation from 79.9\% to 86\% in terms of area under the ROC curve.},
	urldate = {2024-10-16},
	booktitle = {2015 37th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Arevalo, John and González, Fabio A. and Ramos-Pollán, Raúl and Oliveira, Jose L. and Guevara Lopez, Miguel Angel},
	month = aug,
	year = {2015},
	note = {ISSN: 1558-4615},
	keywords = {Breast cancer, Feature extraction, Lesions, Machine learning, Shape, Training},
	pages = {797--800},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\caomi\\Zotero\\storage\\YLJ8BJRU\\7318482.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\TU5EQTWX\\Arevalo et al. - 2015 - Convolutional neural networks for mammography mass lesion classification.pdf:application/pdf},
}

@article{esteva_dermatologist-level_2017,
	title = {Dermatologist-level classification of skin cancer with deep neural networks},
	volume = {542},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature21056},
	doi = {10.1038/nature21056},
	abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
	language = {en},
	number = {7639},
	urldate = {2024-10-16},
	journal = {Nature},
	author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
	month = feb,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Machine learning, Skin cancer},
	pages = {115--118},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\8P5AMYCV\\Esteva et al. - 2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf:application/pdf},
}

@article{guncar_application_2018,
	title = {An application of machine learning to haematological diagnosis},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-18564-8},
	doi = {10.1038/s41598-017-18564-8},
	abstract = {Quick and accurate medical diagnoses are crucial for the successful treatment of diseases. Using machine learning algorithms and based on laboratory blood test results, we have built two models to predict a haematologic disease. One predictive model used all the available blood test parameters and the other used only a reduced set that is usually measured upon patient admittance. Both models produced good results, obtaining prediction accuracies of 0.88 and 0.86 when considering the list of five most likely diseases and 0.59 and 0.57 when considering only the most likely disease. The models did not differ significantly, which indicates that a reduced set of parameters can represent a relevant “fingerprint” of a disease. This knowledge expands the model’s utility for use by general practitioners and indicates that blood test results contain more information than physicians generally recognize. A clinical test showed that the accuracy of our predictive models was on par with that of haematology specialists. Our study is the first to show that a machine learning predictive model based on blood tests alone can be successfully applied to predict haematologic diseases. This result and could open up unprecedented possibilities for medical diagnosis.},
	language = {en},
	number = {1},
	urldate = {2024-10-17},
	journal = {Scientific Reports},
	author = {Gunčar, Gregor and Kukar, Matjaž and Notar, Mateja and Brvar, Miran and Černelč, Peter and Notar, Manca and Notar, Marko},
	month = jan,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Software},
	pages = {411},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\CRN3LBGB\\Gunčar et al. - 2018 - An application of machine learning to haematological diagnosis.pdf:application/pdf},
}

@article{walter_artificial_2023,
	title = {Artificial intelligence in hematological diagnostics: {Game} changer or gadget?},
	volume = {58},
	issn = {0268960X},
	shorttitle = {Artificial intelligence in hematological diagnostics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0268960X22000935},
	doi = {10.1016/j.blre.2022.101019},
	abstract = {The future of clinical diagnosis and treatment of hematologic diseases will inevitably involve the integration of artificial intelligence (AI)-based systems into routine practice to support the hematologists' decision making. Several studies have shown that AI-based models can already be used to automatically differentiate cells, reliably detect malignant cell populations, support chromosome banding analysis, and interpret clinical variants, contributing to early disease detection and prognosis. However, even the best tool can become useless if it is misapplied or the results are misinterpreted. Therefore, in order to comprehensively judge and correctly apply newly developed AI-based systems, the hematologist must have a basic understanding of the general concepts of machine learning. In this review, we provide the hematologist with a comprehensive overview of various machine learning techniques, their current implementations and approaches in different diagnostic subfields (e.g., cytogenetics, molecular genetics), and the limitations and unresolved challenges of the systems.},
	language = {en},
	urldate = {2024-10-17},
	journal = {Blood Reviews},
	author = {Walter, Wencke and Pohlkamp, Christian and Meggendorfer, Manja and Nadarajah, Niroshan and Kern, Wolfgang and Haferlach, Claudia and Haferlach, Torsten},
	month = mar,
	year = {2023},
	pages = {101019},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\KW3IRFDH\\Walter et al. - 2023 - Artificial intelligence in hematological diagnostics Game changer or gadget.pdf:application/pdf},
}

@misc{FDA_artificial_nodate,
	title = {Artificial {Intelligence} and {Machine} {Learning} ({AI}/{ML})-{Enabled} {Medical} {Devices} {\textbar} {FDA}},
	url = {https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices},
	author = {FDA},
	year = {2024},
	urldate = {2024-10-17},
	file = {Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices | FDA:C\:\\Users\\caomi\\Zotero\\storage\\7472SENF\\artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices.html:text/html},
}

@article{ercal_neural_1994,
	title = {Neural network diagnosis of malignant melanoma from color images},
	volume = {41},
	issn = {0018-9294},
	doi = {10.1109/10.312091},
	abstract = {Malignant melanoma is the deadliest form of all skin cancers. Approximately 32,000 new cases of malignant melanoma were diagnosed in 1991 in the United States, with approximately 80\% of patients expected to survive five years [1]. Fortunately, if detected early, even malignant melanoma may be treated successfully. Thus, in recent years, there has been rising interest in the automated detection and diagnosis of skin cancer, particularly malignant melanoma [2]. In this paper, we present a novel neural network approach for the automated separation of melanoma from three benign categories of tumors which exhibit melanoma-like characteristics. Our approach uses discriminant features, based on tumor shape and relative tumor color, that are supplied to an artificial neural network for classification of tumor images as malignant or benign. With this approach, for reasonably balanced training/testing sets, we are able to obtain above 80\% correct classification of the malignant and benign tumors on real skin tumor images.},
	language = {eng},
	number = {9},
	journal = {IEEE transactions on bio-medical engineering},
	author = {Ercal, F. and Chawla, A. and Stoecker, W. V. and Lee, H. C. and Moss, R. H.},
	month = sep,
	year = {1994},
	pmid = {7959811},
	keywords = {Adolescent, Adult, Child, Color, Diagnosis, Computer-Assisted, Humans, Image Interpretation, Computer-Assisted, Melanoma, Models, Biological, Neural Networks, Computer, Skin Neoplasms},
	pages = {837--845},
}

@article{kumar_artificial_2023,
	title = {Artificial {Intelligence} in {Healthcare}: {Review}, {Ethics}, {Trust} {Challenges} \& {Future} {Research} {Directions}},
	volume = {120},
	issn = {0952-1976},
	shorttitle = {Artificial {Intelligence} in {Healthcare}},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197623000787},
	doi = {10.1016/j.engappai.2023.105894},
	abstract = {The use of artificial intelligence (AI) in medicine is beginning to alter current procedures in prevention, diagnosis, treatment, amelioration, cure of disease and other physical and mental impairments. In addition to raising concerns about public trust and ethics, advancements in this new emerging technology have also led to a lot of debate around its integration into healthcare. The objective of this work is to introduce researchers to AI and its medical applications, along with their potential pitfalls, in a comprehensive manner. This paper provides a review of current studies that have investigated how to apply AI methodologies to create a smart predictive maintenance model for the industries of the future. We begin with a brief introduction to AI and a decade’s worth of its advancements across a variety of industries, including smart grids, train transportation, etc., and most recently, healthcare. In this paper, we explore the various applications of AI across various medical specialties, including radiology, dermatology, haematology, ophthalmology, etc. along with the comparative study by employing several key criteria. Finally, it highlights the challenges for large-scale integration of AI in medical systems along with a summary of the ethical, legal, trust, and future implications of AI in healthcare.},
	urldate = {2024-09-27},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Kumar, Pranjal and Chauhan, Siddhartha and Awasthi, Lalit Kumar},
	month = apr,
	year = {2023},
	keywords = {Artificial intelligence, Machine learning, Deep learning, Healthcare, Ethics, Trust},
	pages = {105894},
	annote = {This paper provides a holistic review of AI applications in healthcare, focusing more on the social aspects
},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\UQRDNR6G\\Kumar et al. - 2023 - Artificial Intelligence in Healthcare Review, Ethics, Trust Challenges & Future Research Directions.pdf:application/pdf},
}


@misc{commissioner_fda_2020,
	title = {{FDA} permits marketing of artificial intelligence-based device to detect certain diabetes-related eye problems},
	url = {https://www.fda.gov/news-events/press-announcements/fda-permits-marketing-artificial-intelligence-based-device-detect-certain-diabetes-related-eye},
	abstract = {FDA permits marketing of first medical device to use artificial intelligence to detect greater than a mild level of diabetic retinopathy in the eye of adults who have diabetes.},
	language = {en},
	urldate = {2024-10-21},
	journal = {FDA},
	author = {Commissioner, Office of the},
	month = mar,
	year = {2020},
	note = {Publisher: FDA},
	file = {Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\FGUA8M7G\\fda-permits-marketing-artificial-intelligence-based-device-detect-certain-diabetes-related-eye.html:text/html},
}

@article{abramoff_pivotal_2018,
	title = {Pivotal trial of an autonomous {AI}-based diagnostic system for detection of diabetic retinopathy in primary care offices},
	volume = {1},
	copyright = {2018 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-018-0040-6},
	doi = {10.1038/s41746-018-0040-6},
	abstract = {Artificial Intelligence (AI) has long promised to increase healthcare affordability, quality and accessibility but FDA, until recently, had never authorized an autonomous AI diagnostic system. This pivotal trial of an AI system to detect diabetic retinopathy (DR) in people with diabetes enrolled 900 subjects, with no history of DR at primary care clinics, by comparing to Wisconsin Fundus Photograph Reading Center (FPRC) widefield stereoscopic photography and macular Optical Coherence Tomography (OCT), by FPRC certified photographers, and FPRC grading of Early Treatment Diabetic Retinopathy Study Severity Scale (ETDRS) and Diabetic Macular Edema (DME). More than mild DR (mtmDR) was defined as ETDRS level 35 or higher, and/or DME, in at least one eye. AI system operators underwent a standardized training protocol before study start. Median age was 59 years (range, 22–84 years); among participants, 47.5\% of participants were male; 16.1\% were Hispanic, 83.3\% not Hispanic; 28.6\% African American and 63.4\% were not; 198 (23.8\%) had mtmDR. The AI system exceeded all pre-specified superiority endpoints at sensitivity of 87.2\% (95\% CI, 81.8–91.2\%) ({\textgreater}85\%), specificity of 90.7\% (95\% CI, 88.3–92.7\%) ({\textgreater}82.5\%), and imageability rate of 96.1\% (95\% CI, 94.6–97.3\%), demonstrating AI’s ability to bring specialty-level diagnostics to primary care settings. Based on these results, FDA authorized the system for use by health care providers to detect more than mild DR and diabetic macular edema, making it, the first FDA authorized autonomous AI diagnostic system in any field of medicine, with the potential to help prevent vision loss in thousands of people with diabetes annually. ClinicalTrials.gov NCT02963441},
	language = {en},
	number = {1},
	urldate = {2024-10-21},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Lavin, Philip T. and Birch, Michele and Shah, Nilay and Folk, James C.},
	month = aug,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Eye manifestations},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\8E4LQCE8\\Abràmoff et al. - 2018 - Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in p.pdf:application/pdf},
}

@article{abramoff_improved_2016,
	title = {Improved {Automated} {Detection} of {Diabetic} {Retinopathy} on a {Publicly} {Available} {Dataset} {Through} {Integration} of {Deep} {Learning}},
	volume = {57},
	issn = {1552-5783},
	url = {https://doi.org/10.1167/iovs.16-19964},
	doi = {10.1167/iovs.16-19964},
	abstract = {To compare performance of a deep-learning enhanced algorithm for automated detection of diabetic retinopathy (DR), to the previously published performance of that algorithm, the Iowa Detection Program (IDP)–without deep learning components–on the same publicly available set of fundus images and previously reported consensus reference standard set, by three US Board certified retinal specialists.    We used the previously reported consensus reference standard of referable DR (rDR), defined as International Clinical Classification of Diabetic Retinopathy moderate, severe nonproliferative (NPDR), proliferative DR, and/or macular edema (ME). Neither Messidor-2 images, nor the three retinal specialists setting the Messidor-2 reference standard were used for training IDx-DR version X2.1. Sensitivity, specificity, negative predictive value, area under the curve (AUC), and their confidence intervals (CIs) were calculated.    Sensitivity was 96.8\% (95\% CI: 93.3\%–98.8\%), specificity was 87.0\% (95\% CI: 84.2\%–89.4\%), with 6/874 false negatives, resulting in a negative predictive value of 99.0\% (95\% CI: 97.8\%–99.6\%). No cases of severe NPDR, PDR, or ME were missed. The AUC was 0.980 (95\% CI: 0.968–0.992). Sensitivity was not statistically different from published IDP sensitivity, which had a CI of 94.4\% to 99.3\%, but specificity was significantly better than the published IDP specificity CI of 55.7\% to 63.0\%.    A deep-learning enhanced algorithm for the automated detection of DR, achieves significantly better performance than a previously reported, otherwise essentially identical, algorithm that does not employ deep learning. Deep learning enhanced algorithms have the potential to improve the efficiency of DR screening, and thereby to prevent visual loss and blindness from this devastating disease.},
	number = {13},
	urldate = {2024-10-21},
	journal = {Investigative Ophthalmology \& Visual Science},
	author = {Abràmoff, Michael David and Lou, Yiyue and Erginay, Ali and Clarida, Warren and Amelon, Ryan and Folk, James C. and Niemeijer, Meindert},
	month = oct,
	year = {2016},
	pages = {5200--5206},
	file = {Full Text:C\:\\Users\\caomi\\Zotero\\storage\\E37FMBWZ\\Abràmoff et al. - 2016 - Improved Automated Detection of Diabetic Retinopathy on a Publicly Available Dataset Through Integra.pdf:application/pdf;Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\8SM6TRS2\\article.html:text/html},
}

@article{sayres_using_2019,
	title = {Using a {Deep} {Learning} {Algorithm} and {Integrated} {Gradients} {Explanation} to {Assist} {Grading} for {Diabetic} {Retinopathy}},
	volume = {126},
	issn = {01616420},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0161642018315756},
	doi = {10.1016/j.ophtha.2018.11.016},
	abstract = {Purpose: To understand the impact of deep learning diabetic retinopathy (DR) algorithms on physician readers in computer-assisted settings. Design: Evaluation of diagnostic technology. Participants: One thousand seven hundred ninety-six retinal fundus images from 1612 diabetic patients.
Methods: Ten ophthalmologists (5 general ophthalmologists, 4 retina specialists, 1 retina fellow) read images for DR severity based on the International Clinical Diabetic Retinopathy disease severity scale in each of 3 conditions: unassisted, grades only, or grades plus heatmap. Grades-only assistance comprised a histogram of DR predictions (grades) from a trained deep-learning model. For grades plus heatmap, we additionally showed explanatory heatmaps. Main Outcome Measures: For each experiment arm, we computed sensitivity and speciﬁcity of each reader and the algorithm for different levels of DR severity against an adjudicated reference standard. We also measured accuracy (exact 5-class level agreement and Cohen’s quadratically weighted k), reader-reported conﬁdence (5-point Likert scale), and grading time.
Results: Readers graded more accurately with model assistance than without for the grades-only condition (P {\textless} 0.001). Grades plus heatmaps improved accuracy for patients with DR (P {\textless} 0.001), but reduced accuracy for patients without DR (P ¼ 0.006). Both forms of assistance increased readers’ sensitivity moderate-or-worse DR: unassisted: mean, 79.4\% [95\% conﬁdence interval (CI), 72.3\%e86.5\%]; grades only: mean, 87.5\% [95\% CI, 85.1\%e89.9\%]; grades plus heatmap: mean, 88.7\% [95\% CI, 84.9\%e92.5\%] without a corresponding drop in speciﬁcity (unassisted: mean, 96.6\% [95\% CI, 95.9\%e97.4\%]; grades only: mean, 96.1\% [95\% CI, 95.5\%e 96.7\%]; grades plus heatmap: mean, 95.5\% [95\% CI, 94.8\%e96.1\%]). Algorithmic assistance increased the accuracy of retina specialists above that of the unassisted reader or model alone; and increased grading conﬁdence and grading time across all readers. For most cases, grades plus heatmap was only as effective as grades only. Over the course of the experiment, grading time decreased across all conditions, although most sharply for grades plus heatmap.
Conclusions: Deep learning algorithms can improve the accuracy of, and conﬁdence in, DR diagnosis in an assisted read setting. They also may increase grading time, although these effects may be ameliorated with experience. Ophthalmology 2019;126:552-564 ª 2018 by the American Academy of Ophthalmology. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
	language = {en},
	number = {4},
	urldate = {2024-10-21},
	journal = {Ophthalmology},
	author = {Sayres, Rory and Taly, Ankur and Rahimy, Ehsan and Blumer, Katy and Coz, David and Hammel, Naama and Krause, Jonathan and Narayanaswamy, Arunachalam and Rastegar, Zahra and Wu, Derek and Xu, Shawn and Barb, Scott and Joseph, Anthony and Shumski, Michael and Smith, Jesse and Sood, Arjun B. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
	month = apr,
	year = {2019},
	pages = {552--564},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\78QYR35Z\\Sayres et al. - 2019 - Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\VFY94J54\\S0161642018315756.html:text/html},
}


@article{alomari_automatic_2014,
	title = {Automatic {Detection} and {Quantification} of {WBCs} and {RBCs} {Using} {Iterative} {Structured} {Circle} {Detection} {Algorithm}},
	volume = {2014},
	issn = {1748-6718},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2014/979302},
	doi = {10.1155/2014/979302},
	abstract = {Segmentation and counting of blood cells are considered as an important step that helps to extract features to diagnose some specific diseases like malaria or leukemia. The manual counting of white blood cells (WBCs) and red blood cells (RBCs) in microscopic images is an extremely tedious, time consuming, and inaccurate process. Automatic analysis will allow hematologist experts to perform faster and more accurately. The proposed method uses an iterative structured circle detection algorithm for the segmentation and counting of WBCs and RBCs. The separation of WBCs from RBCs was achieved by thresholding, and specific preprocessing steps were developed for each cell type. Counting was performed for each image using the proposed method based on modified circle detection, which automatically counted the cells. Several modifications were made to the basic (RCD) algorithm to solve the initialization problem, detecting irregular circles (cells), selecting the optimal circle from the candidate circles, determining the number of iterations in a fully dynamic way to enhance algorithm detection, and running time. The validation method used to determine segmentation accuracy was a quantitative analysis that included Precision, Recall, and F-measurement tests. The average accuracy of the proposed method was 95.3\% for RBCs and 98.4\% for WBCs.},
	language = {en},
	number = {1},
	urldate = {2024-10-21},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Alomari, Yazan M. and Sheikh Abdullah, Siti Norul Huda and Zaharatul Azma, Raja and Omar, Khairuddin},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2014/979302},
	pages = {979302},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\H6JB6QHU\\Alomari et al. - 2014 - Automatic Detection and Quantification of WBCs and RBCs Using Iterative Structured Circle Detection.pdf:application/pdf;Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\I9DCSK69\\979302.html:text/html},
}

@article{alferez_automatic_2014,
	title = {Automatic classification of atypical lymphoid {B} cells using digital blood image processing},
	volume = {36},
	copyright = {© 2013 John Wiley \& Sons Ltd},
	issn = {1751-553X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ijlh.12175},
	doi = {10.1111/ijlh.12175},
	abstract = {Introduction There are automated systems for digital peripheral blood (PB) cell analysis, but they operate most effectively in nonpathological blood samples. The objective of this work was to design a methodology to improve the automatic classification of abnormal lymphoid cells. Methods We analyzed 340 digital images of individual lymphoid cells from PB films obtained in the CellaVision DM96:150 chronic lymphocytic leukemia (CLL) cells, 100 hairy cell leukemia (HCL) cells, and 90 normal lymphocytes (N). We implemented the Watershed Transformation to segment the nucleus, the cytoplasm, and the peripheral cell region. We extracted 44 features and then the clustering Fuzzy C-Means (FCM) was applied in two steps for the lymphocyte classification. Results The images were automatically clustered in three groups, one of them with 98\% of the HCL cells. The set of the remaining cells was clustered again using FCM and texture features. The two new groups contained 83.3\% of the N cells and 71.3\% of the CLL cells, respectively. Conclusion The approach has been able to automatically classify with high precision three types of lymphoid cells. The addition of more descriptors and other classification techniques will allow extending the classification to other classes of atypical lymphoid cells.},
	language = {en},
	number = {4},
	urldate = {2024-10-21},
	journal = {International Journal of Laboratory Hematology},
	author = {Alférez, S. and Merino, A. and Mujica, L. E. and Ruiz, M. and Bigorra, L. and Rodellar, J.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ijlh.12175},
	keywords = {Atypical lymphoid cells, automatic cell classification, digital image processing, hematological cytology, morphological analysis, peripheral blood},
	pages = {472--480},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\CSU47VFP\\Alférez et al. - 2014 - Automatic classification of atypical lymphoid B cells using digital blood image processing.pdf:application/pdf;Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\C3PT3PAJ\\ijlh.html:text/html},
}

@article{alferez_automatic_2015,
	title = {Automatic recognition of atypical lymphoid cells from peripheral blood by digital image analysis},
	volume = {143},
	doi = {10.1309/AJCP78IFSTOGZZJN},
	abstract = {Objectives: The objective was the development of a method for the automatic recognition of different types of atypical lymphoid cells. Methods: In the method development, a training set (TS) of 1,500 lymphoid cell images from peripheral blood was used. To segment the images, we used clustering of color components and watershed transformation. In total, 113 features were extracted for lymphocyte recognition by linear discriminant analysis (LDA) with a 10-fold cross-validation over the TS. Then, a new validation set (VS) of 150 images was used, performing two steps: (1) tuning the LDA classifier using the TS and (2) classifying the VS in the different lymphoid cell types. Results: The segmentation algorithm was very effective in separating the cytoplasm, nucleus, and peripheral zone around the cell. From them, descriptive features were extracted and used to recognize the different lymphoid cells. The accuracy for the classification in the TS was 98.07\%. The precision, sensitivity, and specificity values were above 99.7\%, 97.5\%, and 98.6\%, respectively. The accuracy of the classification in the VS was 85.33\%. Conclusions: The method reaches a high precision in the recognition of five different types of lymphoid cells and could allow for the design of a diagnosis support tool in the future. © American Society for Clinical Pathology.},
	number = {2},
	journal = {American Journal of Clinical Pathology},
	author = {Alférez, S. and Merino, A. and Bigorra, L. and Mujica, L. and Ruiz, M. and Rodellar, J.},
	year = {2015},
	keywords = {Atypical lymphoid cells, Automatic cell classification, Digital image processing, Hematologic cytology, Morphologic analysis, Peripheral blood},
	pages = {168--176},
	file = {Full Text:C\:\\Users\\caomi\\Zotero\\storage\\378PH74J\\Alférez et al. - 2015 - Automatic recognition of atypical lymphoid cells from peripheral blood by digital image analysis.pdf:application/pdf;Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\F6HLBP46\\display.html:text/html},
}


@article{cheuque_efficient_2022,
	title = {An {Efficient} {Multi}-{Level} {Convolutional} {Neural} {Network} {Approach} for {White} {Blood} {Cells} {Classification}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/12/2/248},
	doi = {10.3390/diagnostics12020248},
	abstract = {The evaluation of white blood cells is essential to assess the quality of the human immune system; however, the assessment of the blood smear depends on the pathologist’s expertise. Most machine learning tools make a one-level classification for white blood cell classification. This work presents a two-stage hybrid multi-level scheme that efficiently classifies four cell groups: lymphocytes and monocytes (mononuclear) and segmented neutrophils and eosinophils (polymorphonuclear). At the first level, a Faster R-CNN network is applied for the identification of the region of interest of white blood cells, together with the separation of mononuclear cells from polymorphonuclear cells. Once separated, two parallel convolutional neural networks with the MobileNet structure are used to recognize the subclasses in the second level. The results obtained using Monte Carlo cross-validation show that the proposed model has a performance metric of around 98.4\% (accuracy, recall, precision, and F1-score). The proposed model represents a good alternative for computer-aided diagnosis (CAD) tools for supporting the pathologist in the clinical laboratory in assessing white blood cells from blood smear images.},
	language = {en},
	number = {2},
	urldate = {2024-10-21},
	journal = {Diagnostics},
	author = {Cheuque, César and Querales, Marvin and León, Roberto and Salas, Rodrigo and Torres, Romina},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, multi-level classification, multi-source datasets, white blood cells classification},
	pages = {248},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\DP8VDLYP\\Cheuque et al. - 2022 - An Efficient Multi-Level Convolutional Neural Network Approach for White Blood Cells Classification.pdf:application/pdf},
}

@article{wu_hematologist-level_2020,
	title = {A {Hematologist}-{Level} {Deep} {Learning} {Algorithm} ({BMSNet}) for {Assessing} the {Morphologies} of {Single} {Nuclear} {Balls} in {Bone} {Marrow} {Smears}: {Algorithm} {Development}},
	volume = {8},
	copyright = {This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published JMIR Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this copyright and license information must be included.},
	shorttitle = {A {Hematologist}-{Level} {Deep} {Learning} {Algorithm} ({BMSNet}) for {Assessing} the {Morphologies} of {Single} {Nuclear} {Balls} in {Bone} {Marrow} {Smears}},
	url = {https://medinform.jmir.org/2020/4/e15963},
	doi = {10.2196/15963},
	abstract = {Background: Bone marrow aspiration and biopsy remain the gold standard for the diagnosis of hematological diseases despite the development of flow cytometry (FCM) and molecular and gene analyses. However, the interpretation of the results is laborious and operator dependent. Furthermore, the obtained results exhibit inter- and intravariations among specialists. Therefore, it is important to develop a more objective and automated analysis system. Several deep learning models have been developed and applied in medical image analysis but not in the field of hematological histology, especially for bone marrow smear applications. Objective: The aim of this study was to develop a deep learning model (BMSNet) for assisting hematologists in the interpretation of bone marrow smears for faster diagnosis and disease monitoring. Methods: From January 1, 2016, to December 31, 2018, 122 bone marrow smears were photographed and divided into a development cohort (N=42), a validation cohort (N=70), and a competition cohort (N=10). The development cohort included 17,319 annotated cells from 291 high-resolution photos. In total, 20 photos were taken for each patient in the validation cohort and the competition cohort. This study included eight annotation categories: erythroid, blasts, myeloid, lymphoid, plasma cells, monocyte, megakaryocyte, and unable to identify. BMSNet is a convolutional neural network with the YOLO v3 architecture, which detects and classifies single cells in a single model. Six visiting staff members participated in a human-machine competition, and the results from the FCM were regarded as the ground truth. Results: In the development cohort, according to 6-fold cross-validation, the average precision of the bounding box prediction without consideration of the classification is 67.4\%. After removing the bounding box prediction error, the precision and recall of BMSNet were similar to those of the hematologists in most categories. In detecting more than 5\% of blasts in the validation cohort, the area under the curve (AUC) of BMSNet (0.948) was higher than the AUC of the hematologists (0.929) but lower than the AUC of the pathologists (0.985). In detecting more than 20\% of blasts, the AUCs of the hematologists (0.981) and pathologists (0.980) were similar and were higher than the AUC of BMSNet (0.942). Further analysis showed that the performance difference could be attributed to the myelodysplastic syndrome cases. In the competition cohort, the mean value of the correlations between BMSNet and FCM was 0.960, and the mean values of the correlations between the visiting staff and FCM ranged between 0.952 and 0.990. Conclusions: Our deep learning model can assist hematologists in interpreting bone marrow smears by facilitating and accelerating the detection of hematopoietic cells. However, a detailed morphological interpretation still requires trained hematologists.},
	language = {EN},
	number = {4},
	urldate = {2024-10-22},
	journal = {JMIR Medical Informatics},
	author = {Wu, Yi-Ying and Huang, Tzu-Chuan and Ye, Ren-Hua and Fang, Wen-Hui and Lai, Shiue-Wei and Chang, Ping-Ying and Liu, Wei-Nung and Kuo, Tai-Yu and Lee, Cho-Hao and Tsai, Wen-Chiuan and Lin, Chin},
	month = apr,
	year = {2020},
	note = {Company: JMIR Medical Informatics
Distributor: JMIR Medical Informatics
Institution: JMIR Medical Informatics
Label: JMIR Medical Informatics
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e15963},
	file = {Full Text:C\:\\Users\\caomi\\Zotero\\storage\\6R76QG8A\\Wu et al. - 2020 - A Hematologist-Level Deep Learning Algorithm (BMSNet) for Assessing the Morphologies of Single Nucle.pdf:application/pdf;Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\9W7YUZ5X\\e15963.html:text/html},
}

@article{matek_highly_2021,
	title = {Highly accurate differentiation of bone marrow cell morphologies using deep neural networks on a large image data set},
	volume = {138},
	issn = {0006-4971, 1528-0020},
	url = {https://ashpublications.org/blood/article/138/20/1917/477932/Highly-accurate-differentiation-of-bone-marrow},
	doi = {10.1182/blood.2020010568},
	abstract = {Abstract
            Biomedical applications of deep learning algorithms rely on large expert annotated data sets. The classification of bone marrow (BM) cell cytomorphology, an important cornerstone of hematological diagnosis, is still done manually thousands of times every day because of a lack of data sets and trained models. We applied convolutional neural networks (CNNs) to a large data set of 171 374 microscopic cytological images taken from BM smears from 945 patients diagnosed with a variety of hematological diseases. The data set is the largest expert-annotated pool of BM cytology images available in the literature. It allows us to train high-quality classifiers of leukocyte cytomorphology that identify a wide range of diagnostically relevant cell species with high precision and recall. Our CNNs outcompete previous feature-based approaches and provide a proof-of-concept for the classification problem of single BM cells. This study is a step toward automated evaluation of BM cell morphology using state-of-the-art image-classification algorithms. The underlying data set represents an educational resource, as well as a reference for future artificial intelligence–based approaches to BM cytomorphology.},
	language = {en},
	number = {20},
	urldate = {2024-10-22},
	journal = {Blood},
	author = {Matek, Christian and Krappe, Sebastian and Münzenmayer, Christian and Haferlach, Torsten and Marr, Carsten},
	month = nov,
	year = {2021},
	pages = {1917--1927},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\ZQKS4ZVX\\Matek et al. - 2021 - Highly accurate differentiation of bone marrow cell morphologies using deep neural networks on a lar.pdf:application/pdf},
}

@misc{matek_human-level_2019,
	title = {Human-level recognition of blast cells in acute myeloid leukemia with convolutional neural networks},
	url = {http://biorxiv.org/lookup/doi/10.1101/564039},
	doi = {10.1101/564039},
	abstract = {Reliable recognition of malignant white blood cells is a key step in the diagnosis of hematologic malignancies such as Acute Myeloid Leukemia. Microscopic morphological examination of blood cells is usually performed by trained human examiners, making the process tedious, time-consuming and hard to standardise.},
	language = {en},
	urldate = {2024-10-22},
	author = {Matek, Christian and Schwarz, Simone and Spiekermann, Karsten and Marr, Carsten},
	month = feb,
	year = {2019},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\69XZ5QQN\\Matek et al. - 2019 - Human-level recognition of blast cells in acute myeloid leukemia with convolutional neural networks.pdf:application/pdf},
}

@article{zhao_hematologist-level_2020,
	title = {Hematologist-{Level} {Classification} of {Mature} {B}-{Cell} {Neoplasm} {Using} {Deep} {Learning} on {Multiparameter} {Flow} {Cytometry} {Data}},
	volume = {97},
	copyright = {© 2020 The Authors. Cytometry Part A published by Wiley Periodicals LLC. on behalf of International Society for Advancement of Cytometry.},
	issn = {1552-4930},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cyto.a.24159},
	doi = {10.1002/cyto.a.24159},
	abstract = {The wealth of information captured by multiparameter flow cytometry (MFC) can be analyzed by recent methods of computer vision when represented as a single image file. We therefore transformed MFC raw data into a multicolor 2D image by a self-organizing map and classified this representation using a convolutional neural network. By this means, we built an artificial intelligence that is not only able to distinguish diseased from healthy samples, but it can also differentiate seven subtypes of mature B-cell neoplasm. We trained our model with 18,274 cases including chronic lymphocytic leukemia and its precursor monoclonal B-cell lymphocytosis, marginal zone lymphoma, mantle cell lymphoma, prolymphocytic leukemia, follicular lymphoma, hairy cell leukemia, lymphoplasmacytic lymphoma and achieved a weighted F1 score of 0.94 on a separate test set of 2,348 cases. Furthermore, we estimated the trustworthiness of a classification and could classify 70\% of all cases with a confidence of 0.95 and higher. Our performance analyses indicate that particularly for rare subtypes further improvement can be expected when even more samples are available for training. © 2020 The Authors. Cytometry Part A published by Wiley Periodicals LLC. on behalf of International Society for Advancement of Cytometry.},
	language = {en},
	number = {10},
	urldate = {2024-10-24},
	journal = {Cytometry Part A},
	author = {Zhao, Max and Mallesh, Nanditha and Höllein, Alexander and Schabath, Richard and Haferlach, Claudia and Haferlach, Torsten and Elsner, Franz and Lüling, Hannes and Krawitz, Peter and Kern, Wolfgang},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cyto.a.24159},
	keywords = {deep learning, non-Hodgkin lymphoma, self-organizing maps},
	pages = {1073--1080},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\F3FIFQSZ\\Zhao et al. - 2020 - Hematologist-Level Classification of Mature B-Cell Neoplasm Using Deep Learning on Multiparameter Fl.pdf:application/pdf;Snapshot:C\:\\Users\\caomi\\Zotero\\storage\\XF6ZIVZ6\\cyto.a.html:text/html},
}

@article{burlina_automated_2017,
	title = {Automated {Grading} of {Age}-{Related} {Macular} {Degeneration} {From} {Color} {Fundus} {Images} {Using} {Deep} {Convolutional} {Neural} {Networks}},
	volume = {135},
	issn = {2168-6165},
	url = {http://archopht.jamanetwork.com/article.aspx?doi=10.1001/jamaophthalmol.2017.3782},
	doi = {10.1001/jamaophthalmol.2017.3782},
	language = {en},
	number = {11},
	urldate = {2024-10-24},
	journal = {JAMA Ophthalmology},
	author = {Burlina, Philippe M. and Joshi, Neil and Pekala, Michael and Pacheco, Katia D. and Freund, David E. and Bressler, Neil M.},
	month = nov,
	year = {2017},
	pages = {1170},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\BXR29JIU\\Burlina et al. - 2017 - Automated Grading of Age-Related Macular Degeneration From Color Fundus Images Using Deep Convolutio.pdf:application/pdf},
}

@article{schlegl_fully_2018,
	title = {Fully {Automated} {Detection} and {Quantification} of {Macular} {Fluid} in {OCT} {Using} {Deep} {Learning}},
	volume = {125},
	issn = {01616420},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0161642017314240},
	doi = {10.1016/j.ophtha.2017.10.031},
	abstract = {Purpose: Development and validation of a fully automated method to detect and quantify macular ﬂuid in conventional OCT images. Design: Development of a diagnostic modality. Participants: The clinical dataset for ﬂuid detection consisted of 1200 OCT volumes of patients with neovascular age-related macular degeneration (AMD, n ¼ 400), diabetic macular edema (DME, n ¼ 400), or retinal vein occlusion (RVO, n ¼ 400) acquired with Zeiss Cirrus (Carl Zeiss Meditec, Dublin, CA) (n ¼ 600) or Heidelberg Spectralis (Heidelberg Engineering, Heidelberg, Germany) (n ¼ 600) OCT devices.
Methods: A method based on deep learning to automatically detect and quantify intraretinal cystoid ﬂuid (IRC) and subretinal ﬂuid (SRF) was developed. The performance of the algorithm in accurately identifying ﬂuid localization and extent was evaluated against a manual consensus reading of 2 masked reading center graders. Main Outcome Measures: Performance of a fully automated method to accurately detect, differentiate, and quantify intraretinal and SRF using area under the receiver operating characteristics curves, precision, and recall.
Results: The newly designed, fully automated diagnostic method based on deep learning achieved optimal accuracy for the detection and quantiﬁcation of IRC for all 3 macular pathologies with a mean accuracy (AUC) of 0.94 (range, 0.91e0.97), a mean precision of 0.91, and a mean recall of 0.84. The detection and measurement of SRF were also highly accurate with an AUC of 0.92 (range, 0.86e0.98), a mean precision of 0.61, and a mean recall of 0.81, with superior performance in neovascular AMD and RVO compared with DME, which was represented rarely in the population studied. High linear correlation was conﬁrmed between automated and manual ﬂuid localization and quantiﬁcation, yielding an average Pearson’s correlation coefﬁcient of 0.90 for IRC and of 0.96 for SRF.
Conclusions: Deep learning in retinal image analysis achieves excellent accuracy for the differential detection of retinal ﬂuid types across the most prevalent exudative macular diseases and OCT devices. Furthermore, quantiﬁcation of ﬂuid achieves a high level of concordance with manual expert assessment. Fully automated analysis of retinal OCT images from clinical routine provides a promising horizon in improving accuracy and reliability of retinal diagnosis for research and clinical practice in ophthalmology. Ophthalmology 2018;125:549558 ª 2017 by the American Academy of Ophthalmology. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
	language = {en},
	number = {4},
	urldate = {2024-10-24},
	journal = {Ophthalmology},
	author = {Schlegl, Thomas and Waldstein, Sebastian M. and Bogunovic, Hrvoje and Endstraßer, Franz and Sadeghipour, Amir and Philip, Ana-Maria and Podkowinski, Dominika and Gerendas, Bianca S. and Langs, Georg and Schmidt-Erfurth, Ursula},
	month = apr,
	year = {2018},
	keywords = {Minh},
	pages = {549--558},
	file = {PDF:C\:\\Users\\mnguyen6\\Zotero\\storage\\6PDGUCXZ\\Schlegl et al. - 2018 - Fully Automated Detection and Quantification of Macular Fluid in OCT Using Deep Learning.pdf:application/pdf},
}

@article{shibata_development_2018,
	title = {Development of a deep residual learning algorithm to screen for glaucoma from fundus photography},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-33013-w},
	doi = {10.1038/s41598-018-33013-w},
	abstract = {The Purpose of the study was to develop a deep residual learning algorithm to screen for glaucoma from fundus photography and measure its diagnostic performance compared to Residents in Ophthalmology. A training dataset consisted of 1,364 color fundus photographs with glaucomatous indications and 1,768 color fundus photographs without glaucomatous features. A testing dataset consisted of 60 eyes of 60 glaucoma patients and 50 eyes of 50 normal subjects. Using the training dataset, a deep learning algorithm known as Deep Residual Learning for Image Recognition (ResNet) was developed to discriminate glaucoma, and its diagnostic accuracy was validated in the testing dataset, using the area under the receiver operating characteristic curve (AROC). The Deep Residual Learning for Image Recognition was constructed using the training dataset and validated using the testing dataset. The presence of glaucoma in the testing dataset was also confirmed by three Residents in Ophthalmology. The deep learning algorithm achieved significantly higher diagnostic performance compared to Residents in Ophthalmology; with ResNet, the AROC from all testing data was 96.5 (95\% confidence interval [CI]: 93.5 to 99.6)\% while the AROCs obtained by the three Residents were between 72.6\% and 91.2\%.},
	language = {en},
	number = {1},
	urldate = {2024-10-28},
	journal = {Scientific Reports},
	author = {Shibata, Naoto and Tanito, Masaki and Mitsuhashi, Keita and Fujino, Yuri and Matsuura, Masato and Murata, Hiroshi and Asaoka, Ryo},
	month = oct,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Medical research, Minh, Translational research},
	pages = {14665},
	file = {Full Text PDF:C\:\\Users\\mnguyen6\\Zotero\\storage\\E4YSBU3R\\Shibata et al. - 2018 - Development of a deep residual learning algorithm to screen for glaucoma from fundus photography.pdf:application/pdf},
}

@article{thompson_assessment_2020,
	title = {Assessment of a {Segmentation}-{Free} {Deep} {Learning} {Algorithm} for {Diagnosing} {Glaucoma} {From} {Optical} {Coherence} {Tomography} {Scans}},
	volume = {138},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC7042899/},
	doi = {10.1001/jamaophthalmol.2019.5983},
	abstract = {This cross-sectional study discusses the development of a segmentation-free deep learning algorithm for assessment of glaucomatous damage.},
	language = {en},
	number = {4},
	urldate = {2024-10-29},
	journal = {JAMA Ophthalmology},
	author = {Thompson, Atalie C. and Jammal, Alessandro A. and Berchuck, Samuel I. and Mariottoni, Eduardo B. and Medeiros, Felipe A.},
	month = feb,
	year = {2020},
	pmid = {32053142},
	pages = {333},
	file = {Snapshot:C\:\\Users\\mnguyen6\\Zotero\\storage\\JLYEDN87\\PMC7042899.html:text/html},
}

@article{huynh_artificial_2020,
	title = {Artificial intelligence in radiation oncology},
	volume = {17},
	copyright = {2020 Springer Nature Limited},
	issn = {1759-4782},
	url = {https://www.nature.com/articles/s41571-020-0417-8},
	doi = {10.1038/s41571-020-0417-8},
	abstract = {Artificial intelligence (AI) has the potential to fundamentally alter the way medicine is practised. AI platforms excel in recognizing complex patterns in medical data and provide a quantitative, rather than purely qualitative, assessment of clinical conditions. Accordingly, AI could have particularly transformative applications in radiation oncology given the multifaceted and highly technical nature of this field of medicine with a heavy reliance on digital data processing and computer software. Indeed, AI has the potential to improve the accuracy, precision, efficiency and overall quality of radiation therapy for patients with cancer. In this Perspective, we first provide a general description of AI methods, followed by a high-level overview of the radiation therapy workflow with discussion of the implications that AI is likely to have on each step of this process. Finally, we describe the challenges associated with the clinical development and implementation of AI platforms in radiation oncology and provide our perspective on how these platforms might change the roles of radiotherapy medical professionals.},
	language = {en},
	number = {12},
	urldate = {2024-10-29},
	journal = {Nature Reviews Clinical Oncology},
	author = {Huynh, Elizabeth and Hosny, Ahmed and Guthier, Christian and Bitterman, Danielle S. and Petit, Steven F. and Haas-Kogan, Daphne A. and Kann, Benjamin and Aerts, Hugo J. W. L. and Mak, Raymond H.},
	month = dec,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer imaging, Medical imaging, Radiotherapy},
	pages = {771--781},
	file = {PDF:C\:\\Users\\mnguyen6\\Zotero\\storage\\RSAFP5VW\\Huynh et al. - 2020 - Artificial intelligence in radiation oncology.pdf:application/pdf},
}

@article{thompson_artificial_2018,
	title = {Artificial intelligence in radiation oncology: {A} specialty-wide disruptive transformation?},
	volume = {129},
	issn = {0167-8140},
	shorttitle = {Artificial intelligence in radiation oncology},
	url = {https://www.sciencedirect.com/science/article/pii/S0167814018302895},
	doi = {10.1016/j.radonc.2018.05.030},
	abstract = {Artificial intelligence (AI) is emerging as a technology with the power to transform established industries, and with applications from automated manufacturing to advertising and facial recognition to fully autonomous transportation. Advances in each of these domains have led some to call AI the “fourth” industrial revolution [1]. In healthcare, AI is emerging as both a productive and disruptive force across many disciplines. This is perhaps most evident in Diagnostic Radiology and Pathology, specialties largely built around the processing and complex interpretation of medical images, where the role of AI is increasingly seen as both a boon and a threat. In Radiation Oncology as well, AI seems poised to reshape the specialty in significant ways, though the impact of AI has been relatively limited at present, and may rightly seem more distant to many, given the predominantly interpersonal and complex interventional nature of the specialty. In this overview, we will explore the current state and anticipated future impact of AI on Radiation Oncology, in detail, focusing on key topics from multiple stakeholder perspectives, as well as the role our specialty may play in helping to shape the future of AI within the larger spectrum of medicine.},
	number = {3},
	urldate = {2024-10-21},
	journal = {Radiotherapy and Oncology},
	author = {Thompson, Reid F. and Valdes, Gilmer and Fuller, Clifton D. and Carpenter, Colin M. and Morin, Olivier and Aneja, Sanjay and Lindsay, William D. and Aerts, Hugo J. W. L. and Agrimson, Barbara and Deville, Curtiland and Rosenthal, Seth A. and Yu, James B. and Thomas, Charles R.},
	month = dec,
	year = {2018},
	keywords = {Artificial intelligence, Deep learning, Machine learning, Minh, Oncology},
	pages = {421--426},
	file = {Full Text PDF:C\:\\Users\\mnguyen6\\Zotero\\storage\\CF79FN7J\\Thompson et al. - 2018 - Artificial intelligence in radiation oncology A specialty-wide disruptive transformation.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\mnguyen6\\Zotero\\storage\\P9WYGNN8\\S0167814018302895.html:text/html},
}

@article{nguyen_feasibility_2019,
	title = {A feasibility study for predicting optimal radiation therapy dose distributions of prostate cancer patients from patient anatomy using deep learning},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-37741-x},
	doi = {10.1038/s41598-018-37741-x},
	abstract = {With the advancement of treatment modalities in radiation therapy for cancer patients, outcomes have improved, but at the cost of increased treatment plan complexity and planning time. The accurate prediction of dose distributions would alleviate this issue by guiding clinical plan optimization to save time and maintain high quality plans. We have modified a convolutional deep network model, U-net (originally designed for segmentation purposes), for predicting dose from patient image contours of the planning target volume (PTV) and organs at risk (OAR). We show that, as an example, we are able to accurately predict the dose of intensity-modulated radiation therapy (IMRT) for prostate cancer patients, where the average Dice similarity coefficient is 0.91 when comparing the predicted vs. true isodose volumes between 0\% and 100\% of the prescription dose. The average value of the absolute differences in [max, mean] dose is found to be under 5\% of the prescription dose, specifically for each structure is [1.80\%, 1.03\%](PTV), [1.94\%, 4.22\%](Bladder), [1.80\%, 0.48\%](Body), [3.87\%, 1.79\%](L Femoral Head), [5.07\%, 2.55\%](R Femoral Head), and [1.26\%, 1.62\%](Rectum) of the prescription dose. We thus managed to map a desired radiation dose distribution from a patient’s PTV and OAR contours. As an additional advantage, relatively little data was used in the techniques and models described in this paper.},
	language = {en},
	number = {1},
	urldate = {2024-10-29},
	journal = {Scientific Reports},
	author = {Nguyen, Dan and Long, Troy and Jia, Xun and Lu, Weiguo and Gu, Xuejun and Iqbal, Zohaib and Jiang, Steve},
	month = jan,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science, Minh, Radiotherapy, Applied mathematics},
	pages = {1076},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\MVWZ2QFR\\Nguyen et al. - 2019 - A feasibility study for predicting optimal radiation therapy dose distributions of prostate cancer p.pdf:application/pdf},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Minh, Training Image},
	pages = {234--241},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\BG94U2YB\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf},
}

@article{senior_improved_2020,
	title = {Improved protein structure prediction using potentials from deep learning},
	volume = {577},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1923-7},
	doi = {10.1038/s41586-019-1923-7},
	abstract = {Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7. AlphaFold predicts the distances between pairs of residues, is used to construct potentials of mean force that accurately describe the shape of a protein and can be optimized with gradient descent to predict protein structures.},
	language = {en},
	number = {7792},
	urldate = {2024-11-11},
	journal = {Nature},
	author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and Žídek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	month = jan,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Protein structure predictions},
	pages = {706--710},
	file = {PDF:C\:\\Users\\caomi\\Zotero\\storage\\QDQ6PCT5\\Senior et al. - 2020 - Improved protein structure prediction using potentials from deep learning.pdf:application/pdf},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {1476-4687},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1-4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence-the structure prediction component of the 'protein folding problem'8-has been an important open research problem for more than 50 years9. Despite recent progress10-14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {eng},
	number = {7873},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	pmid = {34265844},
	pmcid = {PMC8371605},
	keywords = {Amino Acid Sequence, Computational Biology, Databases, Protein, Deep Learning, Models, Molecular, Neural Networks, Computer, Protein Conformation, Protein Folding, Proteins, Reproducibility of Results, Sequence Alignment},
	pages = {583--589},
	file = {Full Text:C\:\\Users\\caomi\\Zotero\\storage\\DBZB2744\\Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf:application/pdf},
}

@article{abramson_accurate_2024,
	title = {Accurate structure prediction of biomolecular interactions with {AlphaFold} 3},
	volume = {630},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07487-w},
	doi = {10.1038/s41586-024-07487-w},
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2–6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
	language = {en},
	number = {8016},
	urldate = {2024-11-11},
	journal = {Nature},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O’Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and Žemgulytė, Akvilė and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and Žídek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Drug discovery, Machine learning, Protein structure predictions, Structural biology},
	pages = {493--500},
	file = {Full Text PDF:C\:\\Users\\caomi\\Zotero\\storage\\ASMLLBHF\\Abramson et al. - 2024 - Accurate structure prediction of biomolecular interactions with AlphaFold 3.pdf:application/pdf},
}


